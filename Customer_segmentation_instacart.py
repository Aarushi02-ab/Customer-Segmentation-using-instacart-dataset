# -*- coding: utf-8 -*-
"""Instacart_5663439_Finalmatlabfinal.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EuAvSRV4OQ4mAJE9vMNvTx4YlVQsxwWo
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pandas-profiling
# %pip uninstall pandas-profiling
# %pip install ydata-profiling
# %pip install umap-learn
# %pip install tensorflow
# %pip install scikit-learn
# %pip install matplotlib
# %pip install seaborn
# %pip install scikit-fuzzy
# %pip install umap-learn
# %pip install tensorflow scikit-learn umap-learn
# %pip install scikit-learn-extra
# %pip install umap-learn scikit-learn-extra tensorflow_addons
# %pip install tensorflow scikit-learn umap-learn
# %pip install scikit-learn-extra
# %pip install umap-learn scikit-learn-extra tensorflow_addons
# %pip install xgboost
# %pip install tensorflow umap-learn scikit-fuzzy
# %pip install scikit-fuzzy
# %pip install scikit-learn pandas matplotlib seaborn
# %pip install scikit-learn pandas seaborn matplotlib imbalanced-learn
# %pip install umap-learn scikit-fuzzy matplotlib pandas numpy
# %pip install umap-learn scikit-fuzzy pandas numpy

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from ydata_profiling import ProfileReport
from matplotlib import pyplot as plt
import umap.umap_ as umap
import matplotlib.colors as mcolors
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam
import skfuzzy as fuzz
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
import skfuzzy as fuzz
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap
from tensorflow.keras.models import Model
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import umap.umap_ as umap
from sklearn.mixture import GaussianMixture
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam

import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# 1. Imports
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve, average_precision_score
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import XGBClassifier

"""#ReThe dataset is composed of the following tables and fields:

**•	orders:**

order_id: order identifier

user_id: customer identifier

eval_set: which evaluation set this order belongs in (see SET described below)

order_number: the order sequence number for this user (1 = first, n = nth)

order_dow: the day of the week the order was placed on

order_hour_of_day: the hour of the day the order was placed on
days_since_prior: days since the last order, capped at 30 (with NAs for order_number = 1)


**•	products:**
product_id: product identifier

product_name: name of the product

aisle_id: foreign key

department_id: foreign key


**•	aisles:**
aisle_id: aisle identifier

aisle: the name of the aisle


**•	departments:**

department_id: department identifier

department: the name of the department


**•	order_products:**

order_id:

product_id: foreign key

add_to_cart_order: order in which each product was added to cart

reordered: 1 if this product has been ordered by this user in the past, 0 otherwise
where SET is o

"""

# Read CSV files
order_products_prior = pd.read_csv("/content/sample_data/order_products__prior.csv", sep=",")
order_products_train = pd.read_csv("/content/sample_data/order_products__train.csv", sep=",")
orders = pd.read_csv("/content/sample_data/orders.csv", sep=",")
products = pd.read_csv("/content/sample_data/products.csv", sep=",")
aisles = pd.read_csv("/content/sample_data/aisles.csv", sep=",")
departments = pd.read_csv("/content/sample_data/departments.csv", sep=",")

# Get total number of records
total_rows = len(orders)
print(f"Total records in orders.csv: {total_rows}")

"""## **DATA CLEANING**"""

# Strip column with exttra spaces if any
for dataframe in [orders, order_products_train, order_products_prior, products, aisles, departments]:
    dataframe.columns = dataframe.columns.str.strip()

# Search for missing values
print("\n The missing values for the instacart dataset are: ")

for csv, dataframe in [
    ('ORDERS TABLE', orders),
    ('ORDERS PRODUCTS (LAST ORDERS) TABLE', order_products_prior),
    ('PRODUCTS TABLE', products),
    ('AISLES TABLE', aisles),
    ('DEPARTMENTS TABLE', departments),
    ('PREVIOUS ORDER TABLE', order_products_train)
]:
    print(f"\n{csv}:\n{dataframe.isnull().sum()}")

# Removing NA values
datasets_six_CSV = [
    ('ORDERS TABLE', orders),
    ('LAST ORDERS TABLE', order_products_prior),
    ('PRODUCTS TABLE', products),
    ('AISLES TABLE', aisles),
    ('DEPARTMENTS TABLE', departments),
    ('PREVIOUS ORDER TABLE', order_products_train)
]

missing_rows = []
for name, dataframe in datasets_six_CSV:
    for col, missing_count in dataframe.isnull().sum().items():
        missing_rows.append((name, col, missing_count))

missing_df = pd.DataFrame(missing_rows, columns=["Table Name", "Variable Name", "Missing Values"])

print(missing_df)

# Handle missing values
if 'days_since_prior_order' in orders.columns:
    orders['days_since_prior_order'] = orders['days_since_prior_order'].fillna(0)

# Remove Duplicates if any
print('Are there any duplicate rows in the tables?','->')
if orders.duplicated().any() & order_products_prior.duplicated().any() & products.duplicated().any() & aisles.duplicated().any() & departments.duplicated().any() & order_products_train.duplicated().any():
    print('Yes')
else:
    print('No')

# Overview of the clean data
print("\nCleaned Data Overview:")
print("Orders table:", orders.shape)
print("order_products_train table:", order_products_train.shape)
print("order_products_prior table:", order_products_prior.shape)
print("Products table:", products.shape)
print("Aisles table:", aisles.shape)
print("Departments table:", departments.shape)

# Datacasting: Converting columns to suitable data types such as int ot float

orders['order_id'] = orders['order_id'].astype(int)
orders['user_id'] = orders['user_id'].astype(int)
orders['order_number'] = orders['order_number'].astype(int)
orders['order_dow'] = orders['order_dow'].astype(int)
orders['order_hour_of_day'] = orders['order_hour_of_day'].astype(int)
orders['days_since_prior_order'] = orders['days_since_prior_order'].astype(float)
orders['eval_set'] = orders['eval_set'].astype('category')
products['product_id'] = products['product_id'].astype(int)
products['aisle_id'] = products['aisle_id'].astype(int)
products['department_id'] = products['department_id'].astype(int)
order_products_train['order_id'] = order_products_train['order_id'].astype(int)
order_products_train['product_id'] = order_products_train['product_id'].astype(int)
order_products_train['add_to_cart_order'] = order_products_train['add_to_cart_order'].astype(int)
order_products_train['reordered'] = order_products_train['reordered'].astype(int)
order_products_prior['order_id'] = order_products_prior['order_id'].astype(int)
order_products_prior['product_id'] = order_products_prior['product_id'].astype(int)
order_products_prior['add_to_cart_order'] = order_products_prior['add_to_cart_order'].astype(int)
order_products_prior['reordered'] = order_products_prior['reordered'].astype(int)
aisles['aisle_id'] = aisles['aisle_id'].astype(int)
departments['department_id'] = departments['department_id'].astype(int)

print("Data types successfully converted.")

"""## **OUTLIERS REMOVAL**"""

# Decision to remove outliers or not
print("\norder_number stats:")
print(orders['order_number'].describe())

import seaborn as sns
import matplotlib.pyplot as plt

#Visualization for Outliers (Box Plot)
plt.figure(figsize=(10, 4))
sns.boxplot(x=orders['order_number'], color='green')
plt.title('Outlier Check: Order Number per User')
plt.show()

"""## **VISUALIZATIONS**"""

# Using bar graph to show number of orders by the number of users
max_orders_per_user = orders.groupby('user_id')['order_number'].max()

total_numberof_orders, bins = np.histogram(max_orders_per_user, bins=25)

# Beautifying with TABLEAU_COLORS and XKCD_COLORS
all_colors = list(mcolors.TABLEAU_COLORS.values()) + list(mcolors.XKCD_COLORS.values())
colors_of_each_bar = all_colors[:len(total_numberof_orders)]
plt.figure(figsize=(12, 7))
for i in range(len(total_numberof_orders)):
    plt.bar(bins[i], total_numberof_orders[i], width=(bins[1] - bins[0]), color=colors_of_each_bar[i], align='edge')

# Plotting the bar graph
plt.title('Number of Orders per Consumer')
plt.xlabel('Total number of Orders')
plt.ylabel('Number of Users')
plt.tight_layout()
plt.show()

# Number of Orders in different hours of the day
colors = sns.color_palette('Spectral', len(orders['order_hour_of_day'].unique()))
plt.figure(figsize=(10, 4))
sns.countplot(data=orders, x='order_hour_of_day', palette=colors)

# Bar chart
plt.title('Number of orders in different hours of the day')
plt.xlabel('Hours of a Day')
plt.ylabel('Number of Orders')
plt.show()

# Number of orders in a week
plt.figure(figsize=(10, 6))

colors = sns.color_palette('Set3', n_colors = orders['order_dow'].nunique())

# Labelling Sunday = 0 and Saturday = 6
sns.countplot(data=orders, x='order_dow', palette=colors)
plt.title('Number of orders in a week')
plt.xlabel('Day of a week - Sunday = 0, Saturday = 6')
plt.ylabel('Number of Orders')
plt.show()

# Days since first order
num_of_days_after_order = orders['days_since_prior_order']

counts, bins = np.histogram(num_of_days_after_order, bins=30)

colors = cm.plasma(np.linspace(0, 1, len(counts)))

plt.figure(figsize=(10, 4))
for i in range(len(counts)):
    plt.bar(bins[i], counts[i], width=(bins[1] - bins[0]), color=colors[i], align='edge')
# Plot graph
plt.title('Days Since Prior Order Distribution')
plt.xlabel('Days')
plt.ylabel('Number of Orders')
plt.show()

# Calculating Reorder rate
plt.figure(figsize=(12, 7))
plt.title('Reorder Rate by Add to Cart')
sns.lineplot(data = order_products_train, x='add_to_cart_order', y='reordered', color='orange')

plt.xlabel('Add to Cart Order')
plt.ylabel('Rate of Reorder')
plt.show()

rate = order_products_train['reordered'].mean()
print(f"Rate of Re-ordering products: {rate:.4%}")

# top 10 products based on number of orders
top_ten_products = (
    order_products_train.merge(products, on='product_id', how='left')
    ['product_name'].value_counts().head(10).reset_index()
)
top_ten_products.columns = ['Name of the product', 'Quantity']
print(top_ten_products)

# Merging two data frames on the key aisle_id
products_merges_aisle_df = products.merge(aisles, on='aisle_id').merge(departments, on='department_id')
# Merging two data frames on the key product_id
order_merged_products_df = order_products_train.merge(products_merges_aisle_df, on='product_id')
products_merges_aisle_df.head()
order_merged_products_df.head()

# Top 10 departments by the number of orders
plt.figure(figsize=(10,6))
plt.title("Top 10 Departments by Number of Orders")
sns.barplot(x=top_ten_departments.values, y=top_ten_departments.index, palette='viridis')
plt.xlabel("Number of Orders")
plt.ylabel("Name of the Department")
plt.show()

# Top 10 aisles by the number of orders

top_ten_aisles = order_merged_products_df['aisle'].value_counts().head(10)
top_ten_aisles_df = top_ten_aisles.reset_index()
top_ten_aisles_df.columns = ['Aisle', 'Order Count']

# Plot the bar plot
sns.barplot(x='Order Count', y='Aisle', data=top_ten_aisles_df, palette='viridis')
plt.figure(figsize=(10, 5))
plt.title("Top 10 Aisles by Number of Orders")
plt.xlabel("Number of Orders")
plt.ylabel("Name of the Aisle")
plt.show()

"""Organic vs Non-Organic Products"""

# Check for keyword "organic" in product names
order_merged_products_df['is_organic'] = order_merged_products_df['product_name'].str.contains('Organic', case=True)

# Flagging organic to true
organic_counts = order_merged_products_df['is_organic'].value_counts()
labels = ['Non-Organic', 'Organic']

# Plot pie chart
plt.figure(figsize=(7, 7))
plt.pie(organic_counts, labels=labels, autopct='%1.1f%%', startangle=90, colors=['pink', 'lightblue'])
plt.title("Organic vs Non-Organic Products")
plt.show()

organic_counts.head()

orders.head()
#order_products_train.head()

"""# Part 2: Merge All Datasets for Feature Engineering


"""

# Merge orders with order_products
final_dataframe = orders.merge(order_products_train, on='order_id', how='inner')

# Merge with products, aisles, departments
final_dataframe = final_dataframe.merge(products, on='product_id', how='left')
final_dataframe = final_dataframe.merge(aisles, on='aisle_id', how='left')
final_dataframe = final_dataframe.merge(departments, on='department_id', how='left')

print("Full merged dataset shape:", final_dataframe.shape)
print(final_dataframe.head())

# Get total rows
total_rows1 = len(final_dataframe)  # or orders_df.shape[0]
print(f"Total records in orders.csv: {total_rows1}")

# Sample preview of final_dataframe
final_dataframe[['user_id', 'order_id', 'order_number', 'order_dow', 'order_hour_of_day',
         'days_since_prior_order', 'product_id', 'add_to_cart_order',
         'reordered', 'department', 'aisle']].head()


# Group by user to calculate summary stats
customer_features = final_dataframe.groupby('user_id').agg({
    'order_number': 'max',  # total max orders
    'days_since_prior_order': 'mean',  # avg days between orders
    'reordered': 'mean',  # reorder ratio
    'order_hour_of_day': 'mean',  # avg order time
    'order_dow': 'mean',  # avg order day
    'product_id': 'nunique',  # unique products bought
    'add_to_cart_order': 'mean',  # avg cart position
}).reset_index()

# Rename columns for clarity
customer_features.rename(columns={
    'order_number': 'total_max_orders',
    'days_since_prior_order': 'average_days_btw_orders',
    'reordered': 'reorder_mean',
    'order_hour_of_day': 'average_hour_of_day',
    'order_dow': 'average_order_dow',
    'product_id': 'unique_products',
    'add_to_cart_order': 'average_add_to_cart_order'
}, inplace=True)

customer_features.head()

"""**Add categorical features: top department & aisle per user**

Scale Numeric Features
"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
customer_features_scaled = customer_features.copy()

# Select numeric columns to scale
numeric_columns = ['total_max_orders', 'average_days_btw_orders', 'reorder_mean',
            'average_hour_of_day', 'average_order_dow', 'unique_products', 'average_add_to_cart_order']


customer_features_scaled[numeric_columns] = scaler.fit_transform(customer_features[numeric_columns])

"""Encode Categorical Features: K-Means works best with numeric data. For now, we can ignore categorical top_department and top_aisle or encode them later if needed.

# Run K-Means Clustering
"""

inertia_squared_error = []
cluster_range = range(2, 10)

# K-means
for i in cluster_range:
    kmeans_elbow_curve = KMeans(n_clusters=i, random_state=45)
    kmeans_elbow_curve.fit(customer_features_scaled[numeric_columns])
    inertia_squared_error.append(kmeans_elbow_curve.inertia_)

# Elbow curve to identify the number of clusters
plt.figure(figsize=(10, 6))
plt.title('Elbow Method for the value of k')
plt.plot(cluster_range, inertia_squared_error, 'ro-', color='red')
plt.xlabel('Number of clusters (i)')
plt.ylabel('Inertia (Sum of squared error distances)')
plt.show()

"""# SIMPLE K-MEANS IMPLEMENTATION"""

results_simple_kmeans = []

for k in range(2, 10):
    simple_kmeans = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=3000)
    labels_simple_kmeans = simple_kmeans.fit_predict(customer_features_scaled[numeric_columns])

    sil_score_simple_kmeans = silhouette_score(customer_features_scaled[numeric_columns], labels_simple_kmeans, sample_size=3000, random_state=42)
    chi_score_simple_kmeans = calinski_harabasz_score(customer_features_scaled[numeric_columns], labels_simple_kmeans)
    dbi_score_simple_kmeans = davies_bouldin_score(customer_features_scaled[numeric_columns], labels_simple_kmeans)

    results_simple_kmeans.append({
        'k': k,
        'Silhouette Score': sil_score_simple_kmeans,
        'Calinski-Harabasz Index': chi_score_simple_kmeans,
        'Davies-Bouldin Index': dbi_score_simple_kmeans
    })

results_simple_kmeans_df = pd.DataFrame(results_simple_kmeans)
print(results_simple_kmeans_df)

plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.plot(results_simple_kmeans_df['k'], results_simple_kmeans_df['Silhouette Score'], 'bo-')
plt.title('Silhouette Score')
plt.xlabel('clusters')
plt.grid()

plt.subplot(1, 3, 2)
plt.plot(results_simple_kmeans_df['k'], results_simple_kmeans_df['Calinski-Harabasz Index'], 'go-')
plt.title('Calinski-Harabasz Index')
plt.xlabel('clusters')
plt.grid()

plt.subplot(1, 3, 3)
plt.plot(results_simple_kmeans_df['k'], results_simple_kmeans_df['Davies-Bouldin Index'], 'ro-')
plt.title('Davies-Bouldin Index')
plt.xlabel('clusters')
plt.show()

"""# K-MEANS WITH DIMENSIONALITY REDUCTION TECHNIQUE"""

Kmeans_hybrid_approach_user_sample = customer_features.sample(n=3000, random_state=42)
scaler = StandardScaler()
Kmeans_hybrid_X_scaled = scaler.fit_transform(Kmeans_hybrid_approach_user_sample[numeric_columns])

# ----- Autoencoder -----
input_dim = Kmeans_hybrid_X_scaled.shape[1]
encoding_dim = 3
encoding_size = 8

input_layer = Input(shape=(input_dim,))
encoded = Dense(encoding_size, activation='relu')(input_layer)
encoded = Dense(encoding_dim, activation='relu')(encoded)
decoded = Dense(encoding_size, activation='relu')(encoded)
decoded = Dense(input_dim, activation='linear')(decoded)

autoencoder = Model(inputs=input_layer, outputs=decoded)
encoder = Model(inputs=input_layer, outputs=encoded)
autoencoder.compile(optimizer=Adam(learning_rate=0.01), loss='mse')
autoencoder.fit(Kmeans_hybrid_X_scaled, Kmeans_hybrid_X_scaled, epochs=30, batch_size=64, shuffle=True, verbose=0)
X_auto = encoder.predict(Kmeans_hybrid_X_scaled)

# PCA to fit with Kmeans
X_pca = PCA(n_components=3).fit_transform(Kmeans_hybrid_X_scaled)

# UMAP to fit with kmeans
X_umap = umap.UMAP(n_components=3, random_state=42).fit_transform(Kmeans_hybrid_X_scaled)

# t-SNE to fit with Kmeans
X_tsne = TSNE(n_components=3, perplexity=30, random_state=42).fit_transform(Kmeans_hybrid_X_scaled)

#Representations for dimensionality technique
representations = {
    "Autoencoder": X_auto,
    "PCA": X_pca,
    "UMAP": X_umap,
    "t-SNE": X_tsne
}

# Evaluation metrics
results = []

for name, X in representations.items():
    for k in range(2, 10):
        kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)
        labels = kmeans.fit_predict(X)

        sil = silhouette_score(X, labels)
        dbi = davies_bouldin_score(X, labels)
        chi = calinski_harabasz_score(X, labels)

        results.append({
            "Method": name,
            "k": k,
            "Silhouette": sil,
            "DaviesBouldin": dbi,
            "CalinskiHarabasz": chi
        })

# Results DataFrame
Kmeans_hybrid_approach_df = pd.DataFrame(results).sort_values(by=["Method", "k"])
print("K-Means with Autoencoders, PCA, t-SNE, UMAP")
display(Kmeans_hybrid_approach_df)

"""# FUZZY C-MEANS IMPLEMENTATION"""

fuzzycmeans_user_sample = customer_features.sample(n=3000, random_state=42)
X_scaled = scaler.fit_transform(fuzzycmeans_user_sample[numeric_columns])
X_fuzzy = X_scaled.T
X_input = X_scaled


results_fcm = []

for n_clusters in range(2, 10):
    # Run Fuzzy C-Means
    cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(
        X_fuzzy, c=n_clusters, m=2, error=0.005, maxiter=1000, init=None, seed=42
    )

    # Assign clusters based on max membership
    fuzzy_labels = np.argmax(u, axis=0)

    # Metrics (require hard labels)
    sil = silhouette_score(X_input, fuzzy_labels)
    dbi = davies_bouldin_score(X_input, fuzzy_labels)
    chi = calinski_harabasz_score(X_input, fuzzy_labels)

    results_fcm.append({
        "c": n_clusters,
        "Silhouette": sil,
        "DaviesBouldin": dbi,
        "CalinskiHarabasz": chi,
    })

# Convert to DataFrame
fcm_results_df = pd.DataFrame(results_fcm)
display(fcm_results_df)

"""# Fuzzy CMeans + All Dimensionality Reduction Techniques"""

scaler = StandardScaler()

fuzzycmeans_hybrid_user_sample = customer_features.sample(n=3000, random_state=42)
X_scaled = scaler.fit_transform(fuzzycmeans_hybrid_user_sample[numeric_columns])


# ---- Autoencoder ----
input_dim = X_scaled.shape[1]
encoding_dim = 3  # To match PCA/t-SNE/UMAP output shape
encoding_size = 8

input_layer = Input(shape=(input_dim,))
encoded = Dense(encoding_size, activation='relu')(input_layer)
encoded = Dense(encoding_dim, activation='relu')(encoded)
decoded = Dense(encoding_size, activation='relu')(encoded)
decoded = Dense(input_dim, activation='linear')(decoded)

autoencoder = Model(inputs=input_layer, outputs=decoded)
encoder = Model(inputs=input_layer, outputs=encoded)

autoencoder.compile(optimizer=Adam(learning_rate=0.01), loss='mse')
autoencoder.fit(X_scaled, X_scaled, epochs=30, batch_size=64, shuffle=True, verbose=0)

X_auto = encoder.predict(X_scaled)

# Dimensionality Reduction (3D for visualization & clustering)
X_pca = PCA(n_components=3, random_state=42).fit_transform(X_scaled)
X_tsne = TSNE(n_components=3, random_state=42, perplexity=30).fit_transform(X_scaled)
X_umap = umap.UMAP(n_components=3, random_state=42).fit_transform(X_scaled)

# Representations dictionary
representations = {
    "Autoencoder": X_auto,
    "PCA": X_pca,
    "t-SNE": X_tsne,
    "UMAP": X_umap,

}

# Fuzzy C-Means results
all_results = []

for name, X in representations.items():
    print(f"\nRunning Fuzzy C-Means on {name} representation:")
    X_T = X.T  # Fuzzy C-Means expects shape (features, samples)

    for c in range(2, 10):
        # Fuzzy C-Means
        cntr, u, _, _, _, _, fpc = fuzz.cluster.cmeans(
            X_T, c=c, m=2, error=0.005, maxiter=300, init=None, seed=42
        )
        labels = np.argmax(u, axis=0)

        # Metrics
        sil = silhouette_score(X, labels)
        chi = calinski_harabasz_score(X, labels)
        dbi = davies_bouldin_score(X, labels)

        all_results.append({
            "Method": name,
            "Clusters": c,
            "Silhouette": round(sil, 4),
            "Calinski-Harabasz": round(chi, 2),
            "Davies-Bouldin": round(dbi, 4)
        })

        print(f"{name} | c={c}: FPC={fpc:.4f}, Sil={sil:.4f}, CHI={chi:.2f}, DBI={dbi:.4f}")

# Convert results to DataFrame
results_df = pd.DataFrame(all_results)
print("\nFuzzy C-Means Evaluation for Hybrid Approaches:")
display(results_df)

"""Agglomerative Clustering with Ward’s Linkage"""

# Scale numeric data
scaler = StandardScaler()
customer_features_scaled = customer_features.copy()
customer_features_scaled[numeric_columns] = scaler.fit_transform(customer_features[numeric_columns])

# Sample of 3000 users
sample_size = 3000
sample_idx = np.random.choice(customer_features_scaled.index, sample_size, replace=False)
customer_sample = customer_features_scaled.loc[sample_idx, numeric_columns].values

# Employ Agglomerative Clustering for k=2 to 10
results_agg = []

for k in range(2, 10):
    model = AgglomerativeClustering(n_clusters=k, linkage='ward', metric='euclidean')
    labels_agg = model.fit_predict(customer_sample)

    sil = silhouette_score(customer_sample, labels_agg)
    chi = calinski_harabasz_score(customer_sample, labels_agg)
    dbi = davies_bouldin_score(customer_sample, labels_agg)

    results_agg.append({
        "Clusters": k,
        "Silhouette Score": round(sil, 4),
        "Calinski-Harabasz": round(chi, 4),
        "Davies-Bouldin": round(dbi, 4)
    })

# Represnt results as DataFrame
results_agg_df = pd.DataFrame(results_agg)
print("Agglomerative Clustering Evaluation:")
display(results_agg_df)

"""# AGGLOMERATIVE CLUSTERING WITH DIMENSIONALITY TECHNIQUES"""

scaler = StandardScaler()
user_features_scaled = customer_features.copy()
user_features_scaled[numeric_columns] = scaler.fit_transform(customer_features[numeric_columns])

#Sample down to 3000
sample_size = 3000
sample_idx = np.random.choice(user_features_scaled.index, sample_size, replace=False)
user_sample = user_features_scaled.loc[sample_idx, numeric_columns]

# 3a: Autoencoder
input_dim = user_sample.shape[1]
encoding_dim = 3
input_layer = Input(shape=(input_dim,))
encoded = Dense(8, activation='relu')(input_layer)
encoded = Dense(encoding_dim, activation='relu')(encoded)
decoded = Dense(8, activation='relu')(encoded)
decoded = Dense(input_dim, activation='linear')(decoded)

autoencoder = Model(inputs=input_layer, outputs=decoded)
encoder = Model(inputs=input_layer, outputs=encoded)
autoencoder.compile(optimizer=Adam(learning_rate=0.01), loss='mse')
autoencoder.fit(user_sample, user_sample, epochs=30, batch_size=64, shuffle=True, verbose=0)
X_auto = encoder.predict(user_sample)

# 3b: PCA
X_pca = PCA(n_components=3).fit_transform(user_sample)

# 3c: t-SNE
X_tsne = TSNE(n_components=3, perplexity=30, random_state=42).fit_transform(user_sample)

# 3d: UMAP
X_umap = umap.UMAP(n_components=3, random_state=42).fit_transform(user_sample)

# Step 4: Agglomerative Clustering function

def agglomerative_cluster_analysis(X, k_range=range(2, 10)):
    results = []
    for k in k_range:
        model = AgglomerativeClustering(n_clusters=k, linkage='ward', metric='euclidean')
        labels = model.fit_predict(X)

        sil = silhouette_score(X, labels)
        chi = calinski_harabasz_score(X, labels)
        dbi = davies_bouldin_score(X, labels)
# Create representation

        results.append({
            "Clusters": k,
            "Silhouette Score": round(sil, 4),
            "Calinski-Harabasz": round(chi, 2),
            "Davies-Bouldin": round(dbi, 4)
        })
    return pd.DataFrame(results)

# Step 5: Run for all representations

representations = {
    "Autoencoder": X_auto,
    "PCA": X_pca,
    "t-SNE": X_tsne,
    "UMAP": X_umap
}

all_results = []

for name, X in representations.items():
    df_results = agglomerative_cluster_analysis(X)
    df_results['Method'] = name
    all_results.append(df_results)

# Combine all results into one table
summary_df = pd.concat(all_results, ignore_index=True)
summary_df = summary_df[['Method', 'Clusters', 'Silhouette Score', 'Calinski-Harabasz', 'Davies-Bouldin']]

# Summary table
display(summary_df)

"""# **SIMPLE GMM IMPLEMENTATION**

"""

#
gmm_results = []
for k in range(2, 10):  # from 2 to 10 clusters
    gmm = GaussianMixture(n_components=k, random_state=42, covariance_type='full')
    gmm_labels = gmm.fit_predict(user_sample)

    sil_score = silhouette_score(user_sample, gmm_labels)
    chi_score = calinski_harabasz_score(user_sample, gmm_labels)
    dbi_score = davies_bouldin_score(user_sample, gmm_labels)

    gmm_results.append({
        'Clusters': k,
        'Silhouette Score': round(sil_score, 4),
        'Calinski-Harabasz Index': round(chi_score, 2),
        'Davies-Bouldin Index': round(dbi_score, 4)
    })

results_agg_df = pd.DataFrame(gmm_results)
print("Gaussian Mixture Model Evaluation:")
display(results_agg_df)

# Use sample of 3000 users and select numeric features
X_full = customer_features[numeric_columns].dropna()
df_sampled = X_full.sample(n=3000, random_state=42)
X = df_sampled.values

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Use UMAP for 3D embedding
umap_model = umap.UMAP(n_components=3, n_neighbors=15, min_dist=0.1, random_state=42)
X_umap = umap_model.fit_transform(X_scaled)

# Fuzzy C-Means clustering where c=2
cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(
    X_umap.T,           # features x samples
    c=2,                # number of clusters
    m=2.0,
    error=0.005,
    maxiter=1000,
    init=None,
    seed=42
)

cluster_membership = np.argmax(u, axis=0)
df_sampled['cluster'] = cluster_membership

# Segment into cluster profiles
cluster_profiles = df_sampled.groupby('cluster')[numeric_columns].mean()
overall_mean = df_sampled[numeric_columns].mean()
cluster_sizes = df_sampled['cluster'].value_counts().sort_index()
cluster_profiles['Cluster Size'] = cluster_sizes

# Defining behavioral classification

def classify_cluster(row, overall_mean):
    labels = []
    if all(abs(row[col] - overall_mean[col]) <= 0.05 * overall_mean[col] for col in numeric_columns):
        return "Consistent customers"
    if row['total_max_orders'] > overall_mean['total_max_orders'] and row['average_days_btw_orders'] < overall_mean['average_days_btw_orders']:
        labels.append("High-engagement shoppers")
    if row['reorder_mean'] > overall_mean['reorder_mean']:
        labels.append("Retention customers")
    if row['unique_products'] > overall_mean['unique_products'] and row['reorder_mean'] < overall_mean['reorder_mean']:
        labels.append("Variety seekers")

    return ", ".join(labels) if labels else "Unstable shoppers"

cluster_profiles['Description'] = cluster_profiles.apply(lambda row: classify_cluster(row, overall_mean), axis=1)

# Print results

print("Cluster Profiles with Descriptions:")
print(cluster_profiles.round(2))

print("\nOverall Mean:")
print(overall_mean.round(2))

comparison = (cluster_profiles[numeric_columns] - overall_mean).round(2)
print("\nDifference from Overall Mean:")
print(comparison)

# 3D Visualization
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(10,7))
ax = fig.add_subplot(111, projection='3d')
for i in range(2):
    ax.scatter(
        X_umap[cluster_membership == i, 0],
        X_umap[cluster_membership == i, 1],
        X_umap[cluster_membership == i, 2],
        label=f'Cluster {i+1}',
        alpha=0.6
    )

ax.scatter(cntr[:, 0], cntr[:, 1], cntr[:, 2], c='black', marker='x', s=120, label='Centers')
ax.set_title(f'Fuzzy C - Means (c=2) with UMAP in 3D\n')
ax.set_xlabel('UMAP 1'); ax.set_ylabel('UMAP 2'); ax.set_zlabel('UMAP 3')
ax.legend()
plt.show()

"""# *GMM* with all dimensionality Techniques"""

def train_autoencoder(X, encoding_dim=3, epochs=30, batch_size=64):
    input_dim = X.shape[1]
    input_layer = Input(shape=(input_dim,))
    encoded = Dense(8, activation='relu')(input_layer)
    encoded = Dense(encoding_dim, activation='relu')(encoded)
    decoded = Dense(8, activation='relu')(encoded)
    decoded = Dense(input_dim, activation='linear')(decoded)
    autoencoder = Model(inputs=input_layer, outputs=decoded)
    encoder = Model(inputs=input_layer, outputs=encoded)
    autoencoder.compile(optimizer=Adam(learning_rate=0.01), loss='mse')
    autoencoder.fit(X, X, epochs=epochs, batch_size=batch_size, shuffle=True, verbose=0)
    return encoder.predict(X)

# 3. Generate embeddings
X_autoenc = train_autoencoder(X_scaled)
X_pca = PCA(n_components=3, random_state=42).fit_transform(X_scaled)
X_tsne = TSNE(n_components=3, random_state=42, perplexity=30, n_iter=500).fit_transform(X_scaled)
X_umap = umap.UMAP(n_components=3, random_state=42).fit_transform(X_scaled)

embeddings = {
    'Autoencoder': X_autoenc,
    'PCA': X_pca,
    't-SNE': X_tsne,
    'UMAP': X_umap
}

# 4. Loop through k=2 to 9, fit GMM, and evaluate
results_GMM_DR = []

for name, X_emb in embeddings.items():
    for k in range(2, 10):
        gmm = GaussianMixture(n_components=k, random_state=42, covariance_type='full')
        labels = gmm.fit_predict(X_emb)

        sil = silhouette_score(X_emb, labels)
        chi = calinski_harabasz_score(X_emb, labels)
        dbi = davies_bouldin_score(X_emb, labels)

        results_GMM_DR.append({
            'Method': name,
            'Clusters': k,
            'Silhouette Score': round(sil, 4),
            'CHI': round(chi, 2),
            'DBI': round(dbi, 4)
        })
results_agg_df = pd.DataFrame(results_GMM_DR)
#print("Gaussian Mixture Model Evaluation:")
display(results_agg_df)

"""Clustering for ideal model"""

# Calculate total days since first order as a simple proxy for recency
orders_df = pd.read_csv("/content/sample_data/orders.csv", sep=",")
user_last_order = orders_df.groupby('user_id')['days_since_prior_order'].sum().reset_index()
user_last_order.rename(columns={'days_since_prior_order': 'days_since_last_order'}, inplace=True)
# Assuming you have a 'user_features' dataframe
customer_features = customer_features.merge(user_last_order, on='user_id', how='left')
customer_features['churn'] = (customer_features['days_since_last_order'] > 20).astype(int)

# 2. Features and Target
X = customer_features[numeric_columns]
y = customer_features['churn']

# 3. Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42
)

# 4. Scale Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. Handle Imbalance with class weights
neg, pos = np.bincount(y_train)
scale_pos_weight = neg / pos
print("Scale pos weight:", scale_pos_weight)

# Train XGBoost with imbalance handling
xgb = XGBClassifier(
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42,
    scale_pos_weight=scale_pos_weight
)

xgb.fit(X_train_scaled, y_train)

# Prediction
y_pred = xgb.predict(X_test_scaled)
y_proba = xgb.predict_proba(X_test_scaled)[:, 1]

# Classification
print("\n=== Classification Report ===")
print(classification_report(y_test, y_pred))

print("ROC AUC:", roc_auc_score(y_test, y_proba))
print("PR AUC:", average_precision_score(y_test, y_proba))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix: XGBoost with Class Weights")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_proba)
plt.plot(fpr, tpr, label=f'XGBoost (AUC = {roc_auc_score(y_test, y_proba):.2f})')
plt.plot([0,1],[0,1],'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.grid(True)
plt.show()

prec, rec, _ = precision_recall_curve(y_test, y_proba)
plt.plot(rec, prec, label=f'PR AUC = {average_precision_score(y_test, y_proba):.2f}')
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend()
plt.grid(True)
plt.show()